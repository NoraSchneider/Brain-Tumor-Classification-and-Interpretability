{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0088a9",
   "metadata": {},
   "source": [
    "# Variational Autoencoder with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import get_img_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from project3Lib.transforms import EnhanceContrast\n",
    "from masked_dataset import MaskedDataset\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, SGD, RMSprop, lr_scheduler\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from project3Lib.VAE import * \n",
    "import project3Lib.utils as utils\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69d22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device state:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6571be",
   "metadata": {},
   "source": [
    "## Loading variational autoencoder training data (larger dataset for trasnfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b76a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_transform = []\n",
    "train_transform += [\n",
    "            transforms.Resize(128),             # resize shortest side to 128 pixels\n",
    "            transforms.CenterCrop(128),         # crop longest side to 128 pixels at center\n",
    "            transforms.ToTensor(),               # convert PIL image to tensor\n",
    "            EnhanceContrast(reduce_dim=False), \n",
    "            #transforms.Grayscale()\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "transform = [ transforms.RandomRotation(90), transforms.RandomHorizontalFlip(), transforms.ColorJitter() ]\n",
    "transform+= train_transform\n",
    "transform = transforms.Compose(transform)\n",
    "\n",
    "train_transform = transforms.Compose(train_transform)\n",
    "\n",
    "dataset = ImageFolder(root='./data/tl_dataset', transform=transform)\n",
    "dataset2 = ImageFolder(root='./data/tl_dataset', transform=train_transform)\n",
    "\n",
    "dataset = ConcatDataset([dataset,dataset2] )\n",
    "batch_size = 16\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305caeb1",
   "metadata": {},
   "source": [
    "# Classes for Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = VariationalAutoencoder(imgChannels=3).to(device) # GPU\n",
    "vae.train(dataloader, epochs= 30, save='trained_weights/vae_trans_final.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f74cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_loaded  = VariationalAutoencoder(imgChannels=3)\n",
    "vae_loaded.load_state_dict(torch.load('trained_weights/vae_trans_final.torch'))\n",
    "vae_loaded  =vae_loaded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b750f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(vae_loaded, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a828a0",
   "metadata": {},
   "source": [
    "## Perturbed Reconstructions\n",
    "\n",
    "We now take a sample image, get its latent space embedding perturb specific dimension combinations in this embedding and recosntruct images from perturbed embeddings. 30 such reconstruction spaces are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    plot_reconstructed(vae_loaded, dataloader, r0=(-40, 40), r1=(-40, 40), n=10, dims=(i*16,i*16+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0855fd",
   "metadata": {},
   "source": [
    "# Constructing classifier using trained encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dceb58",
   "metadata": {},
   "source": [
    "## Loading project dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contrast enhancing\n",
    "only_enhance = [EnhanceContrast(reduce_dim=False) \n",
    "               #,transforms.Grayscale()\n",
    "               ]\n",
    "\n",
    "#more transformations to increase dataset size and variety\n",
    "transform = [ transforms.RandomRotation(90), transforms.RandomHorizontalFlip(), transforms.ColorJitter() ]\n",
    "transform+= only_enhance\n",
    "\n",
    "#using only unique images\n",
    "input_path = \"data/unique_images\"\n",
    "\n",
    "#concatenating datasets with and without transformations\n",
    "train_dataset,val_dataset, test_dataset = get_img_dataset(only_enhance, data_path=input_path, use_same_transforms = True)\n",
    "train_dataset2,val_dataset2, _ = get_img_dataset(transform,data_path=input_path, use_same_transforms = True)\n",
    "\n",
    "train_dataset = ConcatDataset([train_dataset,train_dataset2, train_dataset2] )\n",
    "val_dataset = ConcatDataset([val_dataset,val_dataset2,val_dataset2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Data Loaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Class sizes{np.unique([y for x,y in train_dataset], return_counts = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede2f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Encoder_Classifier(imgChannels=1, vae_path='trained_weights/vae_trans_final.torch' ).to(device)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "#first train on large dataset\n",
    "model.traindata(dataloader, validloader, epochs=epochs)\n",
    "\n",
    "#then continue training on small dataset\n",
    "model.traindata(trainloader, validloader, epochs=epochs, save=\"trained_weights/vae_transfer_fc_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder_Classifier(imgChannels=3, vae_path='trained_weights/vae_trans_final.torch' )\n",
    "model.load_state_dict(torch.load('trained_weights/vae_transfer_fc_final.pt'))\n",
    "model  =model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8534c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [i for i,j in test_dataset]\n",
    "y_test = [j for i,j in test_dataset]\n",
    "preds = []\n",
    "outs = []\n",
    "for t in x_test:\n",
    "    pred, out = model.predict( t.to(device))\n",
    "    preds.append(pred)\n",
    "    \n",
    "print(f\"Accuracy: {accuracy_score(preds,y_test)}\")\n",
    "print(f\"F1 score: {f1_score(preds,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,preds,normalize=\"true\")\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2703829",
   "metadata": {},
   "source": [
    "# Interpretability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f145e",
   "metadata": {},
   "source": [
    "## Loading Masked Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_nomasks = test_dataset\n",
    "common_transform = [EnhanceContrast(reduce_dim=False)]\n",
    "_,_, test_dataset_mask = get_img_dataset(common_transforms=common_transform, \\\n",
    "                                        data_path=input_path, \\\n",
    "                                        folder_type = MaskedDataset, \\\n",
    "                                        mask_folder=Path(\"data/masks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5e8b2",
   "metadata": {},
   "source": [
    "# Getting Shaply features for the encoder based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b861111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_oneclass(target, dataset):\n",
    "    return [(i,j) for i,j in dataset if j == target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Explainer\n",
    "bg = [i for i,j in train_dataset]\n",
    "bg = torch.stack(bg).to(device)\n",
    "e = shap.DeepExplainer(model, bg)\n",
    "outs = []\n",
    "for i in bg:\n",
    "    pred, out = model.predict(i.to(device))\n",
    "    outs.append((out[0][0].item(), out[0][1].item()))\n",
    "print(f\"Mean values {np.mean([i for i,j in outs])}, {np.mean([j for i,j in outs])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ca4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = []\n",
    "for i, (image,mask,target) in enumerate(test_dataset_mask):\n",
    "    image = image.reshape((1,3,128,128))\n",
    "    pred, out = model.predict(image.to(device).squeeze())\n",
    "    \n",
    "    shap_values = e.shap_values(image)\n",
    "    shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
    "    test_numpy = np.swapaxes(np.swapaxes(image.cpu().numpy(), 1, -1), 1, 2)\n",
    "    print(f\"Image #{i}: True Class {target}, Prediction {pred}, Probabilities {out}\")\n",
    "    \n",
    "    shap.image_plot(shap_numpy, test_numpy)\n",
    "    \n",
    "    predicted_mask = np.copy(shap_values[1].reshape(3,128,128))\n",
    "    mask = mask.reshape((128,128))\n",
    "    mask = torch.stack([mask, mask,mask])\n",
    "    pixels = int(np.sum(mask.numpy().flatten()))\n",
    "    iou = utils.evaluate_interpretability(predicted_mask, mask,pixels)\n",
    "    print(iou)\n",
    "    if target == 1:\n",
    "        ious.append(iou)\n",
    "    if i == 0:\n",
    "        np.save(\"Plots/VAE_SHAP_0\", predicted_mask)\n",
    "    if i == 1:\n",
    "        np.save(\"Plots/VAE_SHAP_1\", predicted_mask)\n",
    "print(f\"Mean IOU: {np.mean(ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983613a",
   "metadata": {},
   "source": [
    "# Integrated Gradients with Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92101eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.encConv6 # choosing conv layer for grad cam -- layer 4 chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca17dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = []\n",
    "for i, (image,mask,target) in enumerate(test_dataset_mask):\n",
    "    data = (image,target)\n",
    "    a, b = utils.plot_grads(data,model, idx = -1,plot=False,grad_type= \"integ_grads\")\n",
    "    if target == 1:\n",
    "        class_1 = a\n",
    "    else:\n",
    "        class_1 = b\n",
    "    predicted_mask = np.copy(class_1.reshape(3,128,128).cpu())\n",
    "    mask = mask.reshape((128,128))\n",
    "    mask = torch.stack([mask, mask,mask])\n",
    "    pixels = int(np.sum(mask.numpy().flatten()))\n",
    "    iou = utils.evaluate_interpretability(predicted_mask, mask,pixels)\n",
    "    print(iou)\n",
    "    if target == 1:\n",
    "        ious.append(iou)\n",
    "    if i == 0:\n",
    "        np.save(\"Plots/VAE_IntGrad_0\", predicted_mask)\n",
    "    if i == 1:\n",
    "        np.save(\"Plots/VAE_IntGrad_1\", predicted_mask)\n",
    "print(f\"The mean iou is {np.mean(ious)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_grads_dataloader(test_dataset, model, grad_type= \"integ_grads\" , plot=True, save_name=\"vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c2e51",
   "metadata": {},
   "source": [
    "# Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56cbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = []\n",
    "for i, (image,mask, target) in enumerate(test_dataset_mask):\n",
    "    data = (image,target)\n",
    "    a, b = utils.plot_grads(data,model, layer=model.encoder.encConv2, plot=False,grad_type= \"grad_cam\")\n",
    "    if target ==1:\n",
    "        class_1 = a\n",
    "    else:\n",
    "        class_1 = b\n",
    "    predicted_mask = np.copy(class_1.detach().cpu().numpy().reshape(128,128))\n",
    "    mask = mask.reshape((128,128))\n",
    "    #mask = torch.stack([mask, mask,mask])\n",
    "    pixels = int(np.sum(mask.numpy().flatten()))\n",
    "    iou = utils.evaluate_interpretability(predicted_mask, mask,pixels)\n",
    "    print(iou)\n",
    "    if target == 1:\n",
    "        ious.append(iou)\n",
    "    if i == 0:\n",
    "        np.save(\"Plots/VAE_GradCam_0\", predicted_mask)\n",
    "    if i == 1:\n",
    "        np.save(\"Plots/VAE_GradCam_1\", predicted_mask)\n",
    "print(f\"The mean iou is {np.mean(ious)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_grads_dataloader(test_dataset, model, layer=model.encoder.encConv2, grad_type= \"grad_cam\" ,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
